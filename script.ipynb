{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset.csv\", delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in data.iterrows():\n",
    "    text = row['text']\n",
    "    origin = row['origin']\n",
    "    destination = row['destination']\n",
    "    detours = row['detours'] if pd.notna(row['detours']) else \"\"\n",
    "    \n",
    "    # Vérifier que le texte est en français et que c'est un itinéraire valide\n",
    "    if origin != \"NOT_FRENCH\" and origin != \"NOT_TRIP\":\n",
    "        # Trouver les positions d'origine et de destination dans le texte\n",
    "        start_origin = text.lower().find(origin)\n",
    "        end_origin = start_origin + len(origin)\n",
    "        start_destination = text.lower().find(destination)\n",
    "        end_destination = start_destination + len(destination)\n",
    "        \n",
    "        # Liste temporaire pour les positions uniques de détour\n",
    "        detour_positions = []\n",
    "        \n",
    "        if detours:\n",
    "            for detour in detours.split(\",\"):\n",
    "                detour = detour.strip()\n",
    "                start_detour = text.lower().find(detour)\n",
    "                end_detour = start_detour + len(detour)\n",
    "                \n",
    "                # Vérifier que le détour ne chevauche pas ORIGIN ou DESTINATION\n",
    "                if start_detour >= 0 and (\n",
    "                    end_detour <= start_origin or start_detour >= end_origin\n",
    "                ) and (\n",
    "                    end_detour <= start_destination or start_detour >= end_destination\n",
    "                ):\n",
    "                    # Vérifier qu'il n'y a pas de chevauchement avec d'autres détours déjà ajoutés\n",
    "                    overlap = any(\n",
    "                        (start < end_detour and end > start_detour)\n",
    "                        for start, end, _ in detour_positions\n",
    "                    )\n",
    "                    if not overlap:\n",
    "                        detour_positions.append((start_detour, end_detour, \"DETOUR\"))\n",
    "        \n",
    "        # Ajouter les annotations d'origine, destination et détours sans chevauchements\n",
    "        if start_origin >= 0 and start_destination >= 0 and end_origin <= start_destination:\n",
    "            entities = [\n",
    "                (start_origin, end_origin, \"ORIGIN\"),\n",
    "                (start_destination, end_destination, \"DESTINATION\")\n",
    "            ] + detour_positions  # Ajouter les détours non-chevauchants\n",
    "\n",
    "            train_data.append((text, {\"entities\": entities}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.add_label(\"ORIGIN\")\n",
    "ner.add_label(\"DESTINATION\")\n",
    "ner.add_label(\"DETOUR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nlp.begin_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gurkan/coding/python/tokenizer/.venv/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"en passant par bessay sur allier coubert calavante...\" with entities \"[(55, 59, 'ORIGIN'), (96, 102, 'DESTINATION'), (15...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/Users/gurkan/coding/python/tokenizer/.venv/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"en passant par beaumont sur vesle et montigny en c...\" with entities \"[(77, 80, 'ORIGIN'), (90, 106, 'DESTINATION'), (15...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/Users/gurkan/coding/python/tokenizer/.venv/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"en faisant un detour par clonas sur vareze buthier...\" with entities \"[(45, 51, 'ORIGIN'), (100, 107, 'DESTINATION'), (2...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itération 0 - Losses: {'ner': 11584.51143947512}\n",
      "Itération 1 - Losses: {'ner': 5326.365459161355}\n",
      "Itération 2 - Losses: {'ner': 4265.298669417285}\n",
      "Itération 3 - Losses: {'ner': 3883.2248929012803}\n",
      "Itération 4 - Losses: {'ner': 3370.4420408958463}\n",
      "Itération 5 - Losses: {'ner': 3073.289576310449}\n",
      "Itération 6 - Losses: {'ner': 2842.5799983024935}\n",
      "Itération 7 - Losses: {'ner': 2705.258543145459}\n",
      "Itération 8 - Losses: {'ner': 2602.3461913991005}\n",
      "Itération 9 - Losses: {'ner': 2441.866069677474}\n",
      "Itération 10 - Losses: {'ner': 2360.8109670044996}\n",
      "Itération 11 - Losses: {'ner': 2288.90263013482}\n",
      "Itération 12 - Losses: {'ner': 2183.6871832169886}\n",
      "Itération 13 - Losses: {'ner': 2142.743837976162}\n",
      "Itération 14 - Losses: {'ner': 2067.8844725613253}\n",
      "Itération 15 - Losses: {'ner': 1996.9890803255498}\n",
      "Itération 16 - Losses: {'ner': 1923.0550091124899}\n",
      "Itération 17 - Losses: {'ner': 1755.896679619108}\n",
      "Itération 18 - Losses: {'ner': 1800.218348164056}\n",
      "Itération 19 - Losses: {'ner': 1790.5218247681667}\n"
     ]
    }
   ],
   "source": [
    "for itn in range(20):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    for text, annotations in train_data:\n",
    "        example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "        nlp.update([example], losses=losses, drop=0.5, sgd=optimizer)\n",
    "    print(f\"Itération {itn} - Losses: {losses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range essayé : 10, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"model_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_french(text):\n",
    "    try:\n",
    "        return detect(text) == \"fr\"\n",
    "    except LangDetectException:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trip_request(text):\n",
    "    if not is_french(text):\n",
    "        return \"NOT_FRENCH\"\n",
    "    \n",
    "    text = unidecode(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    origin, destination = None, None\n",
    "    detours = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORIGIN\":\n",
    "            origin = ent.text\n",
    "        elif ent.label_ == \"DESTINATION\":\n",
    "            destination = ent.text\n",
    "        elif ent.label_ == \"DETOUR\":\n",
    "            detours.append(ent.text)\n",
    "    \n",
    "    if origin and destination:\n",
    "        return (text, origin, destination, detours)\n",
    "    else:\n",
    "        return \"NOT_TRIP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_infos(origin, destination, detours):\n",
    "    print(f'Depart: {origin}')\n",
    "    print(f'Arrivée: {destination}')\n",
    "    detours_sentence = \"\"\n",
    "    for i in range(len(detours)):\n",
    "        if(i == len(detours) - 1):\n",
    "            detours_sentence += detours[i]\n",
    "        else:\n",
    "            detours_sentence += detours[i] + \", \"\n",
    "    print(f'Détours: {detours_sentence if len(detours) > 0 else \"Aucun\"}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrases = [\n",
    "    \"Je veux aller de paris à lyon\",\n",
    "    \"J'aimerai aller de lille à nice\",\n",
    "    \"Voyage de rouen jusqu'à nice\",\n",
    "    \"Quel est le trajet de toulouse à bordeaux ?\",\n",
    "    \"Je veux aller de paris à lyon en passant par nice\",\n",
    "    \"En passant par toulouse, je veux aller de paris à lyon\",\n",
    "    \"J'aime bien les restaurants de paris\",\n",
    "    \"What time is it in Paris ?\",\n",
    "    \"Quel est le trajet de strasbourg à bordeaux ?\",\n",
    "    \"Quel est le trajet de bordeaux à strasbourg en passant par lyon ?\",\n",
    "    \"Comment me rendre à strasbourg depuis nice ?\",\n",
    "    \"En passant par Lyon, j'aimerai aller à Nice depuis Strasbourg\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: Je veux aller de paris à lyon\n",
      "('je veux aller de paris a lyon', 'paris', 'lyon', [])\n",
      "sentence: J'aimerai aller de lille à nice\n",
      "('jaimerai aller de lille a nice', 'lille', 'nice', [])\n",
      "sentence: Voyage de rouen jusqu'à nice\n",
      "('voyage de rouen jusqua nice', 'rouen', 'nice', [])\n",
      "sentence: Quel est le trajet de toulouse à bordeaux ?\n",
      "('quel est le trajet de toulouse a bordeaux ', 'toulouse', 'bordeaux', [])\n",
      "sentence: Je veux aller de paris à lyon en passant par nice\n",
      "('je veux aller de paris a lyon en passant par nice', 'paris', 'lyon', ['nice'])\n",
      "sentence: En passant par toulouse, je veux aller de paris à lyon\n",
      "('en passant par toulouse je veux aller de paris a lyon', 'paris', 'lyon', ['toulouse'])\n",
      "sentence: J'aime bien les restaurants de paris\n",
      "NOT_TRIP\n",
      "sentence: What time is it in Paris ?\n",
      "NOT_FRENCH\n",
      "sentence: Quel est le trajet de strasbourg à bordeaux ?\n",
      "('quel est le trajet de strasbourg a bordeaux ', 'strasbourg', 'bordeaux', [])\n",
      "sentence: Quel est le trajet de bordeaux à strasbourg en passant par lyon ?\n",
      "('quel est le trajet de bordeaux a strasbourg en passant par lyon ', 'bordeaux', 'strasbourg', ['lyon'])\n",
      "sentence: Comment me rendre à strasbourg depuis nice ?\n",
      "NOT_TRIP\n",
      "sentence: En passant par Lyon, j'aimerai aller à Nice depuis Strasbourg\n",
      "NOT_TRIP\n"
     ]
    }
   ],
   "source": [
    "for phrase in test_phrases:\n",
    "    print(f\"sentence: {phrase}\")\n",
    "    print(test_trip_request(phrase.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('en passant par lyon je veux aller de nice a strasbourg',\n",
       " 'nice',\n",
       " 'strasbourg',\n",
       " ['lyon'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"En passant par Lyon, je veux aller de Nice à Strasbourg\"\n",
    "phrase = unidecode(phrase).lower()\n",
    "phrase = re.sub(r'[^\\w\\s]', '', phrase)\n",
    "test_trip_request(phrase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
