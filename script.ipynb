{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset.csv\", delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in data.iterrows():\n",
    "    text = row['text']\n",
    "    origin = row['origin']\n",
    "    destination = row['destination']\n",
    "    detours = row['detours'] if pd.notna(row['detours']) else \"\"\n",
    "    \n",
    "    if origin != \"NOT_FRENCH\" and origin != \"NOT_TRIP\":\n",
    "        start_origin = text.lower().find(origin)\n",
    "        end_origin = start_origin + len(origin)\n",
    "        start_destination = text.lower().find(destination)\n",
    "        end_destination = start_destination + len(destination)\n",
    "        detour_positions = []\n",
    "        \n",
    "        if detours:\n",
    "            for detour in detours.split(\",\"):\n",
    "                detour = detour.strip()\n",
    "                start_detour = text.lower().find(detour)\n",
    "                end_detour = start_detour + len(detour)\n",
    "                \n",
    "                if start_detour >= 0 and (\n",
    "                    end_detour <= start_origin or start_detour >= end_origin\n",
    "                ) and (\n",
    "                    end_detour <= start_destination or start_detour >= end_destination\n",
    "                ):\n",
    "                    overlap = any(\n",
    "                        (start < end_detour and end > start_detour)\n",
    "                        for start, end, _ in detour_positions\n",
    "                    )\n",
    "                    if not overlap:\n",
    "                        detour_positions.append((start_detour, end_detour, \"DETOUR\"))\n",
    "        \n",
    "        if start_origin >= 0 and start_destination >= 0 and end_origin <= start_destination:\n",
    "            entities = [\n",
    "                (start_origin, end_origin, \"ORIGIN\"),\n",
    "                (start_destination, end_destination, \"DESTINATION\")\n",
    "            ] + detour_positions\n",
    "\n",
    "            train_data.append((text, {\"entities\": entities}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gurkan/coding/python/tokenizer/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.add_label(\"ORIGIN\")\n",
    "ner.add_label(\"DESTINATION\")\n",
    "ner.add_label(\"DETOUR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nlp.begin_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gurkan/coding/python/tokenizer/.venv/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"en passant par beaumont sur vesle et montigny en c...\" with entities \"[(77, 80, 'ORIGIN'), (90, 106, 'DESTINATION'), (15...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/Users/gurkan/coding/python/tokenizer/.venv/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"en passant par bessay sur allier coubert calavante...\" with entities \"[(55, 59, 'ORIGIN'), (96, 102, 'DESTINATION'), (15...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/Users/gurkan/coding/python/tokenizer/.venv/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"en faisant un detour par clonas sur vareze buthier...\" with entities \"[(45, 51, 'ORIGIN'), (100, 107, 'DESTINATION'), (2...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itération 0 - Losses: {'ner': 11969.149000367523}\n",
      "Itération 1 - Losses: {'ner': 5189.011326624429}\n",
      "Itération 2 - Losses: {'ner': 4475.519438747725}\n",
      "Itération 3 - Losses: {'ner': 3860.095450588652}\n",
      "Itération 4 - Losses: {'ner': 3476.853364620073}\n",
      "Itération 5 - Losses: {'ner': 3072.7157165446274}\n",
      "Itération 6 - Losses: {'ner': 2914.55863748885}\n",
      "Itération 7 - Losses: {'ner': 2646.7531106838887}\n",
      "Itération 8 - Losses: {'ner': 2564.025007900898}\n",
      "Itération 9 - Losses: {'ner': 2525.6928007663914}\n",
      "Itération 10 - Losses: {'ner': 2378.3990443473813}\n",
      "Itération 11 - Losses: {'ner': 2207.2721563137716}\n",
      "Itération 12 - Losses: {'ner': 2121.6514079720614}\n",
      "Itération 13 - Losses: {'ner': 2025.8923942965098}\n",
      "Itération 14 - Losses: {'ner': 2000.3460953482615}\n",
      "Itération 15 - Losses: {'ner': 1939.918488698682}\n",
      "Itération 16 - Losses: {'ner': 1936.5745040469978}\n",
      "Itération 17 - Losses: {'ner': 1811.6058338802627}\n",
      "Itération 18 - Losses: {'ner': 1917.677065441407}\n",
      "Itération 19 - Losses: {'ner': 1803.4393595372612}\n",
      "Itération 20 - Losses: {'ner': 1618.8940420558388}\n",
      "Itération 21 - Losses: {'ner': 1912.1470291751448}\n",
      "Itération 22 - Losses: {'ner': 1640.8098619719879}\n",
      "Itération 23 - Losses: {'ner': 1704.0464690815359}\n",
      "Itération 24 - Losses: {'ner': 1669.634681115992}\n",
      "Itération 25 - Losses: {'ner': 1624.5212884831028}\n",
      "Itération 26 - Losses: {'ner': 1661.5282863630894}\n",
      "Itération 27 - Losses: {'ner': 1544.33138370607}\n",
      "Itération 28 - Losses: {'ner': 1588.245983763573}\n",
      "Itération 29 - Losses: {'ner': 1597.7209298309283}\n",
      "Itération 30 - Losses: {'ner': 1515.2964564326599}\n",
      "Itération 31 - Losses: {'ner': 1514.5453645776686}\n",
      "Itération 32 - Losses: {'ner': 1516.7365986325567}\n",
      "Itération 33 - Losses: {'ner': 1437.5334442777666}\n",
      "Itération 34 - Losses: {'ner': 1585.5593840398412}\n",
      "Itération 35 - Losses: {'ner': 1554.9723740641614}\n",
      "Itération 36 - Losses: {'ner': 1518.0656100793904}\n",
      "Itération 37 - Losses: {'ner': 1461.5915897131445}\n",
      "Itération 38 - Losses: {'ner': 1530.3255344688994}\n",
      "Itération 39 - Losses: {'ner': 1561.6487203023744}\n",
      "Itération 40 - Losses: {'ner': 1403.082547736327}\n",
      "Itération 41 - Losses: {'ner': 1478.9673821906829}\n",
      "Itération 42 - Losses: {'ner': 1438.5318282374694}\n",
      "Itération 43 - Losses: {'ner': 1499.9378078504428}\n",
      "Itération 44 - Losses: {'ner': 1451.4064251857671}\n",
      "Itération 45 - Losses: {'ner': 1592.429448614652}\n",
      "Itération 46 - Losses: {'ner': 1575.0546000950828}\n",
      "Itération 47 - Losses: {'ner': 1397.0789279732835}\n",
      "Itération 48 - Losses: {'ner': 1390.9333361133395}\n",
      "Itération 49 - Losses: {'ner': 1334.185833764194}\n",
      "Itération 50 - Losses: {'ner': 1413.6180660438033}\n",
      "Itération 51 - Losses: {'ner': 1427.2812702471113}\n",
      "Itération 52 - Losses: {'ner': 1434.1145421629574}\n",
      "Itération 53 - Losses: {'ner': 1323.8780260746607}\n",
      "Itération 54 - Losses: {'ner': 1455.3886869596408}\n",
      "Itération 55 - Losses: {'ner': 1423.8662370633035}\n",
      "Itération 56 - Losses: {'ner': 1418.0865213594939}\n",
      "Itération 57 - Losses: {'ner': 1390.3677782031748}\n",
      "Itération 58 - Losses: {'ner': 1329.8784914896664}\n",
      "Itération 59 - Losses: {'ner': 1466.6754874954277}\n"
     ]
    }
   ],
   "source": [
    "for itn in range(60):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    for text, annotations in train_data:\n",
    "        example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "        nlp.update([example], losses=losses, drop=0.5, sgd=optimizer)\n",
    "    print(f\"Itération {itn} - Losses: {losses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range essayé : 10, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"model_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_french(text):\n",
    "    try:\n",
    "        return detect(text) == \"fr\"\n",
    "    except LangDetectException:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trip_request(text):\n",
    "    if not is_french(text):\n",
    "        return \"NOT_FRENCH\"\n",
    "    \n",
    "    text = unidecode(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    origin, destination = None, None\n",
    "    detours = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORIGIN\":\n",
    "            origin = ent.text\n",
    "        elif ent.label_ == \"DESTINATION\":\n",
    "            destination = ent.text\n",
    "        elif ent.label_ == \"DETOUR\":\n",
    "            detours.append(ent.text)\n",
    "    \n",
    "    if origin and destination:\n",
    "        return (text, origin, destination, detours)\n",
    "    else:\n",
    "        return \"NOT_TRIP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_infos(origin, destination, detours):\n",
    "    print(f'Depart: {origin}')\n",
    "    print(f'Arrivée: {destination}')\n",
    "    detours_sentence = \"\"\n",
    "    for i in range(len(detours)):\n",
    "        if(i == len(detours) - 1):\n",
    "            detours_sentence += detours[i]\n",
    "        else:\n",
    "            detours_sentence += detours[i] + \", \"\n",
    "    print(f'Détours: {detours_sentence if len(detours) > 0 else \"Aucun\"}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrases = [\n",
    "    \"Je veux aller de paris à lyon\",\n",
    "    \"J'aimerai aller de lille à nice\",\n",
    "    \"Voyage de rouen jusqu'à nice\",\n",
    "    \"Quel est le trajet de toulouse à bordeaux ?\",\n",
    "    \"Je veux aller de paris à lyon en passant par nice\",\n",
    "    \"En passant par toulouse, je veux aller de paris à lyon\",\n",
    "    \"J'aime bien les restaurants de paris\",\n",
    "    \"What time is it in Paris ?\",\n",
    "    \"Quel est le trajet de strasbourg à bordeaux ?\",\n",
    "    \"Quel est le trajet de bordeaux à strasbourg en passant par lyon ?\",\n",
    "    \"Comment me rendre à strasbourg depuis nice ?\",\n",
    "    \"En passant par Lyon, j'aimerai aller à Nice depuis Strasbourg\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: Je veux aller de paris à lyon\n",
      "('je veux aller de paris a lyon', 'paris', 'lyon', [])\n",
      "sentence: J'aimerai aller de lille à nice\n",
      "('jaimerai aller de lille a nice', 'lille', 'nice', [])\n",
      "sentence: Voyage de rouen jusqu'à nice\n",
      "('voyage de rouen jusqua nice', 'rouen', 'nice', [])\n",
      "sentence: Quel est le trajet de toulouse à bordeaux ?\n",
      "('quel est le trajet de toulouse a bordeaux ', 'toulouse', 'bordeaux', [])\n",
      "sentence: Je veux aller de paris à lyon en passant par nice\n",
      "('je veux aller de paris a lyon en passant par nice', 'paris', 'lyon', ['nice'])\n",
      "sentence: En passant par toulouse, je veux aller de paris à lyon\n",
      "('en passant par toulouse je veux aller de paris a lyon', 'paris', 'lyon', ['toulouse'])\n",
      "sentence: J'aime bien les restaurants de paris\n",
      "NOT_TRIP\n",
      "sentence: What time is it in Paris ?\n",
      "NOT_FRENCH\n",
      "sentence: Quel est le trajet de strasbourg à bordeaux ?\n",
      "('quel est le trajet de strasbourg a bordeaux ', 'strasbourg', 'bordeaux', [])\n",
      "sentence: Quel est le trajet de bordeaux à strasbourg en passant par lyon ?\n",
      "('quel est le trajet de bordeaux a strasbourg en passant par lyon ', 'bordeaux', 'strasbourg', ['lyon'])\n",
      "sentence: Comment me rendre à strasbourg depuis nice ?\n",
      "NOT_TRIP\n",
      "sentence: En passant par Lyon, j'aimerai aller à Nice depuis Strasbourg\n",
      "NOT_TRIP\n"
     ]
    }
   ],
   "source": [
    "for phrase in test_phrases:\n",
    "    print(f\"sentence: {phrase}\")\n",
    "    print(test_trip_request(phrase.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('voyage de rouen jusqua nice', 'rouen', 'nice', [])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"Voyage de rouen jusqu'à nice\"\n",
    "phrase = unidecode(phrase).lower()\n",
    "phrase = re.sub(r'[^\\w\\s]', '', phrase)\n",
    "test_trip_request(phrase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
