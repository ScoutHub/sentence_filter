{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset.csv\", delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in data.iterrows():\n",
    "    text = row['text']\n",
    "    origin = row['origin']\n",
    "    destination = row['destination']\n",
    "    detours = row['detours'] if pd.notna(row['detours']) else \"\"\n",
    "    \n",
    "    # Vérifier que le texte est en français et que c'est un itinéraire valide\n",
    "    if origin != \"NOT_FRENCH\" and origin != \"NOT_TRIP\":\n",
    "        # Trouver les positions d'origine et de destination dans le texte\n",
    "        start_origin = text.lower().find(origin)\n",
    "        end_origin = start_origin + len(origin)\n",
    "        start_destination = text.lower().find(destination)\n",
    "        end_destination = start_destination + len(destination)\n",
    "        \n",
    "        # Liste temporaire pour les positions uniques de détour\n",
    "        detour_positions = []\n",
    "        \n",
    "        if detours:\n",
    "            for detour in detours.split(\",\"):\n",
    "                detour = detour.strip()\n",
    "                start_detour = text.lower().find(detour)\n",
    "                end_detour = start_detour + len(detour)\n",
    "                \n",
    "                # Vérifier que le détour ne chevauche pas ORIGIN ou DESTINATION\n",
    "                if start_detour >= 0 and (\n",
    "                    end_detour <= start_origin or start_detour >= end_origin\n",
    "                ) and (\n",
    "                    end_detour <= start_destination or start_detour >= end_destination\n",
    "                ):\n",
    "                    # Vérifier qu'il n'y a pas de chevauchement avec d'autres détours déjà ajoutés\n",
    "                    overlap = any(\n",
    "                        (start < end_detour and end > start_detour)\n",
    "                        for start, end, _ in detour_positions\n",
    "                    )\n",
    "                    if not overlap:\n",
    "                        detour_positions.append((start_detour, end_detour, \"DETOUR\"))\n",
    "        \n",
    "        # Ajouter les annotations d'origine, destination et détours sans chevauchements\n",
    "        if start_origin >= 0 and start_destination >= 0 and end_origin <= start_destination:\n",
    "            entities = [\n",
    "                (start_origin, end_origin, \"ORIGIN\"),\n",
    "                (start_destination, end_destination, \"DESTINATION\")\n",
    "            ] + detour_positions  # Ajouter les détours non-chevauchants\n",
    "\n",
    "            train_data.append((text, {\"entities\": entities}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.add_label(\"ORIGIN\")\n",
    "ner.add_label(\"DESTINATION\")\n",
    "ner.add_label(\"DETOUR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nlp.begin_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gurkan/coding/python/tokenizer/.venv/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Je veux voyager à autruy sur juine de ger\" with entities \"[(12, 15, 'ORIGIN'), (18, 34, 'DESTINATION')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/Users/gurkan/coding/python/tokenizer/.venv/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"En passant par moncel sur vair, st paul en cornill...\" with entities \"[(114, 117, 'ORIGIN'), (127, 133, 'DESTINATION'), ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/Users/gurkan/coding/python/tokenizer/.venv/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"En faisant un détour par st martin du tertre, je v...\" with entities \"[(17, 20, 'ORIGIN'), (64, 73, 'DESTINATION'), (25,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/Users/gurkan/coding/python/tokenizer/.venv/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"En passant par crezancy, magny lormes, mussey sur ...\" with entities \"[(3, 8, 'ORIGIN'), (126, 131, 'DESTINATION'), (15,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itération 0 - Losses: {'ner': 4299.90973671333}\n",
      "Itération 1 - Losses: {'ner': 985.861847242741}\n",
      "Itération 2 - Losses: {'ner': 935.811436770639}\n",
      "Itération 3 - Losses: {'ner': 722.0249990224086}\n",
      "Itération 4 - Losses: {'ner': 708.8160397640829}\n"
     ]
    }
   ],
   "source": [
    "for itn in range(5):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    for text, annotations in train_data:\n",
    "        example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "        nlp.update([example], losses=losses, drop=0.5, sgd=optimizer)\n",
    "    print(f\"Itération {itn} - Losses: {losses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"model_trajectoire_detour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_french(text):\n",
    "    try:\n",
    "        return detect(text) == \"fr\"\n",
    "    except LangDetectException:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trip_request(text):\n",
    "    if not is_french(text):\n",
    "        return \"NOT_FRENCH\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    origin, destination = None, None\n",
    "    detours = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORIGIN\":\n",
    "            origin = ent.text\n",
    "        elif ent.label_ == \"DESTINATION\":\n",
    "            destination = ent.text\n",
    "        elif ent.label_ == \"DETOUR\":\n",
    "            detours.append(ent.text)\n",
    "    \n",
    "    if origin and destination:\n",
    "        return (text, origin, destination, detours)\n",
    "    else:\n",
    "        return \"NOT_TRIP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrases = [\n",
    "    \"Je veux aller de paris à lyon\",\n",
    "    \"J'aimerai aller de lille à nice\",\n",
    "    \"Voyage de rouen jusqu'à nice\",\n",
    "    \"Quel est le trajet de toulouse à bordeaux ?\",\n",
    "    \"Je veux aller de paris à lyon en passant par nice\",\n",
    "    \"En passant par toulouse, je veux aller de paris à lyon\",\n",
    "    \"J'aime bien les restaurants de paris\",\n",
    "    \"What time is it in Paris ?\",\n",
    "    \"Quel est le trajet de strasbourg à bordeaux ?\",\n",
    "    \"Quel est le trajet de bordeaux à strasbourg en passant par lyon ?\",\n",
    "    \"Comment me rendre à strasbourg depuis nice ?\",\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: Je veux aller de paris à lyon\n",
      "('Je veux aller de paris à lyon', 'paris', 'lyon', [])\n",
      "sentence: J'aimerai aller de lille à nice\n",
      "(\"J'aimerai aller de lille à nice\", 'lille', 'nice', [])\n",
      "sentence: Voyage de rouen jusqu'à nice\n",
      "(\"Voyage de rouen jusqu'à nice\", 'rouen', 'nice', [])\n",
      "sentence: Quel est le trajet de toulouse à bordeaux ?\n",
      "('Quel est le trajet de toulouse à bordeaux ?', 'toulouse', 'bordeaux', [])\n",
      "sentence: Je veux aller de paris à lyon en passant par nice\n",
      "('Je veux aller de paris à lyon en passant par nice', 'paris', 'lyon', ['nice'])\n",
      "sentence: En passant par toulouse, je veux aller de paris à lyon\n",
      "('En passant par toulouse, je veux aller de paris à lyon', 'paris', 'lyon', ['toulouse'])\n",
      "sentence: J'aime bien les restaurants de paris\n",
      "NOT_TRIP\n",
      "sentence: What time is it in Paris ?\n",
      "NOT_FRENCH\n",
      "sentence: Quel est le trajet de strasbourg à bordeaux ?\n",
      "('Quel est le trajet de strasbourg à bordeaux ?', 'strasbourg', 'bordeaux', [])\n",
      "sentence: Quel est le trajet de bordeaux à strasbourg en passant par lyon ?\n",
      "('Quel est le trajet de bordeaux à strasbourg en passant par lyon ?', 'bordeaux', 'strasbourg', ['lyon'])\n",
      "sentence: Comment me rendre à strasbourg depuis nice ?\n",
      "NOT_TRIP\n"
     ]
    }
   ],
   "source": [
    "for phrase in test_phrases:\n",
    "    print(f\"sentence: {phrase}\")\n",
    "    print(test_trip_request(phrase))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Je suis à parcey et je veux aller à lyon', 'suis', 'lyon', [])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_trip_request(\"Je suis à parcey et je veux aller à lyon\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
